{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark to Compute KPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and set up Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = SparkConf().setAppName(\"SparkTraining\").setMaster(\"local[*]\")\n",
    "ctx = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish the connection. If this doesn't work, comment this and use local files (snippit below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "opendata_leisureRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.opendatabcn-leisure\") \\\n",
    "    .load() \\\n",
    "    .rdd\n",
    "\n",
    "opendata_incomeRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.opendatabcn-income\") \\\n",
    "    .load() \\\n",
    "    .rdd\n",
    "\n",
    "idealista = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.idealista\") \\\n",
    "    .load() \\\n",
    "\n",
    "lookupRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.lookup_tables\") \\\n",
    "    .load() \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the local files if the connection doesn't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idealista = ctx.read.json('./json/idealista.json')\n",
    "lookupRDD = ctx.read.json('./json/lookup.json').rdd\n",
    "opendata_incomeRDD = ctx.read.json('./json/opendata_income.json').rdd\n",
    "opendata_leisureRDD = ctx.read.json('./json/opendata_leisure.json').rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the schema of idealista to be later used when building a df out of the transformed RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idealistaSchema = idealista.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    \"\"\"\n",
    "    Flattens a nested tuple containing property data, idealista data, and other features into a single tuple.\n",
    "    \"\"\"\n",
    "    val = [t[1][0][0], t[0][0], t[0][1]]\n",
    "    for v in t[1][0][1]:\n",
    "        val.append(v)\n",
    "    return tuple(val + [t[1][1][0], t[1][1][1]])\n",
    "\n",
    "def partition_hash_neighbourhood_id(id):\n",
    "    \"\"\"\n",
    "    Returns a partition hash for a given neighborhood ID, based on its first character.\n",
    "    \"\"\"\n",
    "    val = ord(id[:1])  # Convert first character to its ASCII value\n",
    "    return val % 2\n",
    "\n",
    "def x_later_than_y(x_date, y_date):\n",
    "    \"\"\"\n",
    "    Checks if x_date is later than y_date. Dates are in the format \"YYYY_MM_DD\".\n",
    "    \"\"\"\n",
    "    xy, xm, xd = x_date.split(\"_\")\n",
    "    yy, ym, yd = y_date.split(\"_\")\n",
    "    if yy > xy: return False\n",
    "    elif ym > xm: return False\n",
    "    elif yd > xd: return False\n",
    "    else: return True\n",
    "\n",
    "def reconcile_idealista(x, y):\n",
    "    \"\"\"\n",
    "    Returns the latest record between x and y based on their date fields.\n",
    "    \"\"\"\n",
    "    if x_later_than_y(x[1], y[1]):\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "def get_partition_id(id, n=2):\n",
    "    \"\"\"\n",
    "    Returns a partition ID for a given ID by hashing it and taking modulo n.\n",
    "    \"\"\"\n",
    "    val = hash(id)\n",
    "    return val % n\n",
    "\n",
    "def merge_income_dict_count(x, y):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries, summing the values for the same keys.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    xk = list(x.keys())\n",
    "    yk = list(y.keys())\n",
    "    for key in set(xk + yk):  # Union of keys from both dictionaries\n",
    "        if key in xk and key in yk:\n",
    "            out[key] = x[key] + y[key]\n",
    "        elif key in yk:\n",
    "            out[key] = y[key]\n",
    "        elif key in xk:\n",
    "            out[key] = x[key]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lookupRDD`\n",
    "\n",
    "- Selects the neighborhood, the reconciled neighborhood name, and the numerical identifier.\n",
    "- Removes all duplicates from the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "luRDD = lookupRDD.map(lambda t: (t['neighborhood'], (t['neighborhood_id'], t['neighborhood_n_reconciled'])))\\\n",
    "    .reduceByKey(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIRDD`\n",
    "\n",
    "- Selects the name of the neighborhood, the year, and the measured income level.\n",
    "- Filters out all neighborhoods labeled \"No consta\" as they do not contain useful data.\n",
    "- Partitions the data by calling `get_partition_id` on the neighborhood name.\n",
    "- Joins the data with the `lookupRDD` to obtain the neighborhood ID.\n",
    "- Transforms the data so that the neighborhood ID and reconciled name become the key of the tuple, while retaining the year and income as value entries in a dictionary.\n",
    "- Merges all dictionaries corresponding to each neighborhood into a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openIRDD = opendata_incomeRDD.map(lambda t: (t['Nom_Barri'], (t['Any'], t['√çndex RFD Barcelona = 100'])))\\\n",
    "    .filter(lambda t: t[0] != \"No consta\")\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: (t[1][1], {t[1][0][0]: t[1][0][1]}))\\\n",
    "    .reduceByKey(lambda x, y: {**x, **y})\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openLRDD`\n",
    "\n",
    "- Ignores the 'created' field as it does not accurately reflect when the amenity was actually built.\n",
    "- Selects the neighborhood and the type of amenity as a key, incrementing a count for each amenity.\n",
    "- Filters out all neighborhoods with empty names.\n",
    "- Immediately joins the data with the `lookupRDD` to simplify subsequent operations.\n",
    "- Excludes neighborhoods in the `lookupRDD` that are not present in the leisure data. A left outer join is used to ensure this.\n",
    "- Rearranges the data so the key becomes a tuple containing the ID, reconciled name, and amenity type, with a value of 1 for each amenity.\n",
    "- Performs a `reduceByKey` operation to count the total number of amenities per neighborhood, using the tuple `(neighborhood, amenity)` as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openLRDD = opendata_leisureRDD.map(lambda t: (t['addresses_neighborhood_name'],\n",
    "                                              (t['secondary_filters_name'], 1)))\\\n",
    "    .filter(lambda t: t[0] != '')\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: ((t[1][1][0], t[1][1][1]), {str(t[1][0][0]): t[1][0][1]}))\\\n",
    "    .reduceByKey(lambda x, y: merge_income_dict_count(x, y))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joinOpenRDD`\n",
    "\n",
    "- Performs a full outer join on the neighborhood ID between `openLRDD` and `openIRDD`.\n",
    "- Accounts for keys that may not appear in either dataset by setting any `None` values to empty dictionaries.\n",
    "- Utilizes `mapValues` to transform the values without changing the keys, preserving the partition information, which aids in subsequent joins with `idealistaRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinOpenRDD = openLRDD.fullOuterJoin(openIRDD)\\\n",
    "    .mapValues(lambda t: ({} if t[0] == None else t[0], t[1]))\\\n",
    "    .mapValues(lambda t: (t[0], {} if t[1] == None else t[1]))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ilRDD`\n",
    "\n",
    "- Creates rows with the property ID as the key and saves the neighborhood name along with other data.\n",
    "- Performs a `reduceByKey` operation on the property ID to remove duplicates, retaining the record with the latest date through the `reconcile_idealista` function.\n",
    "- Transforms the key to the neighborhood name for joining with `lookupRDD`.\n",
    "- Filters out records where the neighborhood name is not a string.\n",
    "- Joins with `lookupRDD` to get the neighborhood ID and reconciled name.\n",
    "- Maps the key to a tuple containing the reconciled neighborhood ID and name.\n",
    "- Joins the RDD with `joinOpenRDD`, which contains both leisure and income data.\n",
    "- Flattens all values into a single, non-nested tuple.\n",
    "- Checks for duplicates based on the latest scraping date and retains the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilRDD = idealista.rdd.map(lambda t: (t['propertyCode'], (t['neighborhood'], t['scrap_date'], t[1:])))\\\n",
    "    .reduceByKey(lambda x, y: reconcile_idealista(x, y))\\\n",
    "    .map(lambda t: (t[1][0], (t[0], t[1][2])))\\\n",
    "    .filter(lambda t: isinstance(t[0], str))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: (t[1][1], t[1][0]))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(joinOpenRDD)\\\n",
    "    .map(lambda t: flatten(t)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run first part of the pipeline and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joinOpenRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "schema_list = [StructField(\"propertyCode\", StringType(), False)] # Not nullable as it is an ID\n",
    "schema_list.append(StructField(\"NeighbourhoodID\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline\n",
    "schema_list.append(StructField(\"NeighbourhoodName\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline\n",
    "for i, field in enumerate(idealistaSchema):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        schema_list.append(field)\n",
    "schema_list.append(StructField(\"LeisureDict\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline (at most an empty dict)\n",
    "schema_list.append(StructField(\"IncomeDict\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline (at most an empty dict)\n",
    "schema = StructType(schema_list)\n",
    "df = ctx.createDataFrame(data=ilRDD.collect(), schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the KPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 1: Information Rating of Listing\n",
    "This KPI measures the \"commonness\" of the information provided in each property listing. It is defined as the sum of the following features:\n",
    "- `hasVideo`: Presence of a video in the listing, normalized by the total number of listings with a video.\n",
    "- `has360`: Presence of a 360-degree view in the listing, normalized by the total number of listings with a 360-degree view.\n",
    "- `hasPlan`: Presence of a floor plan in the listing, normalized by the total number of listings with a floor plan.\n",
    "- `has3DTour`: Presence of a 3D tour in the listing, normalized by the total number of listings with a 3D tour.\n",
    "- `hasStaging`: Presence of staging in the listing, normalized by the total number of listings with staging.\n",
    "- `numPhotos`: Number of photos in the listing, normalized by the average number of photos across all listings.\n",
    "- `showAddress`: Whether the address is shown in the listing, normalized by the total number of listings that show the address.\n",
    "\n",
    "Additionally, the neighborhood name and price are captured for each listing. These will be used for joining datasets and creating meaningful plots in Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_value(avg, t, feature):\n",
    "    \"\"\"\n",
    "    Sets the value of a specific feature in the average dictionary.\n",
    "    \"\"\"\n",
    "    avg['total_' + feature] = 1 if t[feature] else 0\n",
    "    return avg\n",
    "\n",
    "def init_averages(t):\n",
    "    \"\"\"\n",
    "    Initializes the average dictionary for each listing.\n",
    "    \"\"\"\n",
    "    avg = {}\n",
    "    features = [\"hasVideo\", \"has360\", \"hasPlan\", \"has3DTour\", \"hasStaging\", \"showAddress\"]\n",
    "    for feature in features:\n",
    "        avg = set_value(avg, t, feature)\n",
    "    avg[\"avgNumPhotos\"] = t[\"numPhotos\"]\n",
    "    avg[\"count\"] = 1\n",
    "    return avg\n",
    "\n",
    "def calc_totals(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the total counts for all features.\n",
    "    \"\"\"\n",
    "    return {key: x[key] + y[key] for key in x.keys()}\n",
    "\n",
    "def calc_averages(t):\n",
    "    \"\"\"\n",
    "    Calculates the average number of photos.\n",
    "    \"\"\"\n",
    "    t[\"avgNumPhotos\"] /= t['count']\n",
    "    return t\n",
    "\n",
    "def calc_kpi1(t):\n",
    "    \"\"\"\n",
    "    Calculates the KPI1 value for each listing.\n",
    "    \"\"\"\n",
    "    kpi1 = sum(t[1][0][0][key] / t[1][1][key] for key in t[1][0][0].keys())\n",
    "    return (t[1][0][1], kpi1, t[1][0][2], t[1][0][3])\n",
    "\n",
    "# Initialize RDD from DataFrame\n",
    "KPI1rdd = df.rdd.map(lambda t: ('key', (init_averages(t), t['propertyCode'], t['NeighbourhoodName'], t['price'])))\\\n",
    "    .cache()\n",
    "\n",
    "# Calculate averages\n",
    "averages = KPI1rdd.mapValues(lambda t: t[0])\\\n",
    "    .reduceByKey(calc_totals)\\\n",
    "    .mapValues(calc_averages).cache()\n",
    "\n",
    "# Calculate KPI1 and join with averages\n",
    "KPI1rdd = KPI1rdd.join(averages)\\\n",
    "    .map(calc_kpi1)\n",
    "\n",
    "# Collect and write results to CSV\n",
    "kpi1 = KPI1rdd.collect()\n",
    "features = ['PropertyID', 'InformationScore', 'District', 'Price']\n",
    "kpi1.sort()\n",
    "with open('KPIs/kpi1.csv', 'w') as f:\n",
    "    write = csv.writer(f, lineterminator='\\n')\n",
    "    write.writerow(features)\n",
    "    write.writerows(kpi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 2: Amenities-to-Price Ratio\n",
    "This KPI measures the ratio of neighborhood amenities to the average listing price. It is calculated both for selling and renting properties. The KPI is defined as:\n",
    "\n",
    "\n",
    "$\\text{KPI 2} = \\frac{\\text{Average Listing Price}}{\\text{Total Number of Amenities} + 1}$\n",
    "\n",
    "The '+1' in the denominator is to avoid division by zero.\n",
    "\n",
    "- `Average Listing Price`: The average price of all listings in the neighborhood for a specific type of operation (selling or renting).\n",
    "- `Total Number of Amenities`: The total count of different types of amenities in the neighborhood.\n",
    "\n",
    "The output features are 'District', 'Type of Offer', and 'Price to Leisure Ratio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def leisure_str2amount(lstr):\n",
    "    \"\"\"\n",
    "    Converts a leisure string into the total count of amenities.\n",
    "    \"\"\"\n",
    "    amount = 0\n",
    "    if lstr != '{}':\n",
    "        for item in lstr.strip(\"{}\").split(\",\"):\n",
    "            key, val = item.split(\"=\")\n",
    "            amount += int(val)\n",
    "    return amount\n",
    "\n",
    "def init_kpi2(t):\n",
    "    \"\"\"\n",
    "    Initializes the KPI2 values for each listing.\n",
    "    \"\"\"\n",
    "    return ((t['NeighbourhoodName'], t['operation']), (t['price'], leisure_str2amount(t['LeisureDict']), 1))\n",
    "\n",
    "# Initialize and calculate KPI2\n",
    "KPI2rdd = df.rdd.map(init_kpi2)\\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1], x[2] + y[2]))\\\n",
    "    .map(lambda t: (t[0][0], t[0][1], (t[1][0] / t[1][2]) / (t[1][1] + 1)))  # Adding 1 to avoid division by zero\n",
    "\n",
    "# Collect and write results to CSV\n",
    "kpi2 = KPI2rdd.collect()\n",
    "features = ['District', 'Type of offer', 'Price to Leisure Ratio']\n",
    "kpi2.sort()\n",
    "with open('KPIs/kpi2.csv', 'w') as f:\n",
    "    write = csv.writer(f, lineterminator='\\n')\n",
    "    write.writerow(features)\n",
    "    write.writerows(kpi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 3: Monthly Listings per District\n",
    "This KPI measures the number of listings posted per month for each district (municipality). It is defined as the count of listings grouped by:\n",
    "\n",
    "- `District`: The name of the district or municipality.\n",
    "- `Month`: The month in which the listings were posted.\n",
    "\n",
    "The output features are 'District', 'Month', and 'Number of Listings'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_kpi3(t):\n",
    "    \"\"\"\n",
    "    Initializes the KPI3 values for each listing.\n",
    "    \"\"\"\n",
    "    return ((t['NeighbourhoodName'], t['scrap_date'][:-3]), 1)\n",
    "\n",
    "# Initialize and calculate KPI3\n",
    "KPI3rdd = df.rdd.map(init_kpi3)\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .map(lambda t: (t[0][0], t[0][1], t[1]))\n",
    "\n",
    "# Collect and write results to CSV\n",
    "kpi3 = KPI3rdd.collect()\n",
    "features = ['District', 'Month', 'Number of Listings']\n",
    "kpi3.sort()\n",
    "with open('KPIs/kpi3.csv', 'w') as f:\n",
    "    write = csv.writer(f, lineterminator='\\n')\n",
    "    write.writerow(features)\n",
    "    write.writerows(kpi3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

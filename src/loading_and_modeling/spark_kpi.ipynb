{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = SparkConf().setAppName(\"SparkTraining\").setMaster(\"local[*]\")\n",
    "ctx = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Establish the connection. If this doesn't work, uncomment this and use local files (snippit below)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "opendata_leisureRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.opendatabcn-leisure\") \\\n",
    "    .load() \\\n",
    "    .rdd\n",
    "\n",
    "opendata_incomeRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.opendatabcn-income\") \\\n",
    "    .load() \\\n",
    "    .rdd\n",
    "\n",
    "idealista = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.idealista\") \\\n",
    "    .load() \\\n",
    "\n",
    "lookupRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.lookup_tables\") \\\n",
    "    .load() \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the local files if the connection doesn't work\n",
    "idealista = ctx.read.json('./json/idealista.json')\n",
    "lookupRDD = ctx.read.json('./json/lookup.json').rdd\n",
    "opendata_incomeRDD = ctx.read.json('./json/opendata_income.json').rdd\n",
    "opendata_leisureRDD = ctx.read.json('./json/opendata_leisure.json').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the schema of idealista to be later used when building a df out of the transformed RDD\n",
    "idealistaSchema = idealista.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    \"\"\"\n",
    "    Transforms a nested tuple into a single flattened tuple, reorganizing the data for a specific structure.\n",
    "    \n",
    "    The resulting tuple is organized as follows:\n",
    "    (propertyID, neighbourhoodID, idealistaData (with each feature appended), leisureDict, IncomeDict).\n",
    "    \n",
    "    Parameters:\n",
    "    t (tuple): The nested tuple to be flattened.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A flattened tuple with values ordered as specified.\n",
    "    \"\"\"\n",
    "    val = [t[1][0][0], t[0][0], t[0][1]]\n",
    "    for v in t[1][0][1]:\n",
    "        val.append(v)\n",
    "    return tuple(val + [t[1][1][0], t[1][1][1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def partition_hash_neighbourhood_id(id):\n",
    "    \"\"\"\n",
    "    Computes a hash partition based on the first character of the given identifier, dividing the data into two partitions.\n",
    "    \n",
    "    The function extracts the first character of the identifier, treats it as an integer, and calculates its modulo 2 \n",
    "    to determine which partition it belongs to. This way, it categorizes the data into two partitions, facilitating\n",
    "    parallel processing or grouped operations later on.\n",
    "    \n",
    "    Parameters:\n",
    "    id (str): The identifier whose first character is to be used for partitioning.\n",
    "    \n",
    "    Returns:\n",
    "    int: The partition number (0 or 1) the identifier belongs to.\n",
    "    \"\"\"\n",
    "    val = int(id[:1])\n",
    "    return val % 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def x_later_than_y(x_date, y_date):\n",
    "    xy, xm, xd = x_date.split(\"_\")\n",
    "    yy, ym, yd = y_date.split(\"_\")\n",
    "    if yy > xy: return False\n",
    "    elif ym > xm: return False\n",
    "    elif yd > xd: return False\n",
    "    else: return True\n",
    "\n",
    "\n",
    "def reconcile_idealista(x,y):\n",
    "    if x_later_than_y(x[1], y[1]):\n",
    "        return x\n",
    "    else: return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_partition_id(id, n=2):\n",
    "    val = hash(id)\n",
    "    return val % n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_income_dict_count(x,y):\n",
    "    out = {}\n",
    "    xk = list(x.keys())\n",
    "    yk = list(y.keys())\n",
    "    for key in set(xk + yk):\n",
    "        if key in xk and key in yk:\n",
    "            out[key] = x[key] + y[key]\n",
    "        elif key in yk:\n",
    "            out[key] = y[key]\n",
    "        elif key in xk:\n",
    "            out[key] = x[key]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "luRDD = lookupRDD.map(lambda t: (t['neighborhood'], (t['neighborhood_id'], t['neighborhood_n_reconciled'])))\\\n",
    "    .reduceByKey(lambda x, y: x)\n",
    "    ##select the neighbourhood, the reconciled neighbourhood namme, and the numerical identifier.\n",
    "    ##Remove all duplicates from the lookup table\n",
    "\n",
    "openIRDD = opendata_incomeRDD.map(lambda t: (t['Nom_Barri'], (t['Any'], t['Índex RFD Barcelona = 100'])))\\\n",
    "    .filter(lambda t: t[0] != \"No consta\")\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: (t[1][1], {t[1][0][0]: t[1][0][1]}))\\\n",
    "    .reduceByKey(lambda x, y: {**x, **y})\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\n",
    "    ## Select the name of the neighbourhood, the year and, the measured income level\n",
    "    ## Filter out all the \"No consta\" neighbourhoods as they do not contain data.\n",
    "    ## Join the data with the lookup table to get the neighborhood ID.\n",
    "    ## Make the neighborhood ID and the reconciled name the key of the tuple, while also keeping the year and income as value as dict entries.\n",
    "    ## mapValues not possible as key is being changed\n",
    "    ## Combine all dictionairies for each neigbhorhood.\n",
    "\n",
    "\n",
    "openLRDD = opendata_leisureRDD.map(lambda t: (t['addresses_neighborhood_name'],\n",
    "                                              (t['secondary_filters_name'], 1)))\\\n",
    "    .filter(lambda t: t[0] != '')\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: ((t[1][1][0], t[1][1][1]), {str(t[1][0][0]): t[1][0][1]}))\\\n",
    "    .reduceByKey(lambda x, y: merge_income_dict_count(x, y))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\n",
    "    ## It doesn't make sense to filter for 'created' as it does not reflect the true moment of when the amenity was built.\n",
    "    ## Select the neigbhorhood, the type of amenity as a key, and count 1 for that amenity.\n",
    "    ## filter out all neighborhoods with values ''\n",
    "    ## Immediately join with the lookup table so the later operations are easier\n",
    "    ## filter out all the neighborhoods in the lookup table that were not in the leisure data. Opposite does not have to happen\n",
    "    ## sice we are doing a leftOuterJoin.\n",
    "    ## Rearrange the data so that the key is (ID, reconciled name, amenity) and the value is 1.\n",
    "    ## mapValues not used as we are changing keys\n",
    "    ## Count the amount of amenities per neighborhood by doing a reduceKey on (neighborhood, amenity) as key.\n",
    "\n",
    "joinOpenRDD = openLRDD.fullOuterJoin(openIRDD)\\\n",
    "    .mapValues(lambda t: ({} if t[0] == None else t[0], t[1]))\\\n",
    "    .mapValues(lambda t: (t[0], {} if t[1] == None else t[1]))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .cache()\n",
    "    ## Join both the open data sources on the neighbourhood ID\n",
    "    ## As we are doing an FullJoin we have to account for some keys not appearing in either set\n",
    "    ## Thus we set any None values to empty dictionairies.\n",
    "    ## Use mapValues as we are not changing which keeps the partition information valid, helping for the join in idealistaRDD\n",
    "\n",
    "ilRDD = idealista.rdd.map(lambda t: (t['propertyCode'], (t['neighborhood'], t['scrap_date'], t[1:])))\\\n",
    "    .reduceByKey(lambda x, y: reconcile_idealista(x, y))\\\n",
    "    .map(lambda t: (t[1][0], (t[0], t[1][2])))\\\n",
    "    .filter(lambda t: isinstance(t[0], str))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: (t[1][1], t[1][0]))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(joinOpenRDD)\\\n",
    "    .map(lambda t: flatten(t)).cache()\n",
    "    ## Create rows with as a key the property ID and save the neighbourhood name\n",
    "    ## Reduce by key on the property ID to remove duplicates (the one with latest date is kept as reconciliation)\n",
    "    ## Map the key to be the neighbourhood name so it can be joined with the lookup data\n",
    "    ## join with the lookup table\n",
    "    ## Map the key so that it is (neighbourhoodID, neighbourhoodName) reconciled ofcourse...\n",
    "    ## join with the opendata both Leisure and Income\n",
    "    ## flatten all values to a single non-nested tuple\n",
    "## replace duplicate with check on latest scraping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Q3298502', 'Montbau'),\n",
       "  ({'Biblioteques': 3,\n",
       "    \"Sales d'estudi\": 3,\n",
       "    'Biblioteques municipals': 1,\n",
       "    'Àrees de jocs infantils': 4,\n",
       "    \"Casals d'avis\": 1,\n",
       "    'Parcs i jardins': 4,\n",
       "    'WiFi BCN': 1},\n",
       "   {2007: '85.5',\n",
       "    2008: '88.6',\n",
       "    2009: '80.0',\n",
       "    2010: '82.2',\n",
       "    2011: '71.1',\n",
       "    2012: '76.4',\n",
       "    2013: '71.5',\n",
       "    2014: '70.0',\n",
       "    2015: '72.3',\n",
       "    2016: '82.2',\n",
       "    2017: '79.8'}))]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Run first part of the pipeline and cache it. Otherwise, the python worker crashes :3\n",
    "joinOpenRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --->  StructField(address,StringType,true)\n",
      "2 --->  StructField(bathrooms,LongType,true)\n",
      "3 --->  StructField(country,StringType,true)\n",
      "4 --->  StructField(detailedType,StructType(List(StructField(subTypology,StringType,true),StructField(typology,StringType,true))),true)\n",
      "5 --->  StructField(distance,StringType,true)\n",
      "6 --->  StructField(district,StringType,true)\n",
      "7 --->  StructField(exterior,BooleanType,true)\n",
      "8 --->  StructField(externalReference,StringType,true)\n",
      "9 --->  StructField(floor,StringType,true)\n",
      "10 --->  StructField(has360,BooleanType,true)\n",
      "11 --->  StructField(has3DTour,BooleanType,true)\n",
      "12 --->  StructField(hasLift,BooleanType,true)\n",
      "13 --->  StructField(hasPlan,BooleanType,true)\n",
      "14 --->  StructField(hasStaging,BooleanType,true)\n",
      "15 --->  StructField(hasVideo,BooleanType,true)\n",
      "16 --->  StructField(latitude,DoubleType,true)\n",
      "17 --->  StructField(longitude,DoubleType,true)\n",
      "18 --->  StructField(municipality,StringType,true)\n",
      "19 --->  StructField(neighborhood,StringType,true)\n",
      "20 --->  StructField(newDevelopment,BooleanType,true)\n",
      "21 --->  StructField(newDevelopmentFinished,BooleanType,true)\n",
      "22 --->  StructField(numPhotos,LongType,true)\n",
      "23 --->  StructField(operation,StringType,true)\n",
      "24 --->  StructField(parkingSpace,StructType(List(StructField(hasParkingSpace,BooleanType,true),StructField(isParkingSpaceIncludedInPrice,BooleanType,true),StructField(parkingSpacePrice,DoubleType,true))),true)\n",
      "25 --->  StructField(price,DoubleType,true)\n",
      "26 --->  StructField(priceByArea,DoubleType,true)\n",
      "27 --->  StructField(propertyCode,StringType,true)\n",
      "28 --->  StructField(propertyType,StringType,true)\n",
      "29 --->  StructField(province,StringType,true)\n",
      "30 --->  StructField(rooms,LongType,true)\n",
      "31 --->  StructField(scrap_date,StringType,true)\n",
      "32 --->  StructField(showAddress,BooleanType,true)\n",
      "33 --->  StructField(size,DoubleType,true)\n",
      "34 --->  StructField(status,StringType,true)\n",
      "35 --->  StructField(suggestedTexts,StructType(List(StructField(subtitle,StringType,true),StructField(title,StringType,true))),true)\n",
      "36 --->  StructField(thumbnail,StringType,true)\n",
      "37 --->  StructField(topNewDevelopment,BooleanType,true)\n",
      "38 --->  StructField(url,StringType,true)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "schema_list = [StructField(\"propertyCode\", StringType(), False)] #Not nullable as it is an ID\n",
    "schema_list.append(StructField(\"NeighbourhoodID\", StringType(), True)) #Nullable, but should not have any null values due to the pipeline\n",
    "schema_list.append(StructField(\"NeighbourhoodName\", StringType(), True)) #Nullable, but should not have any null values due to the pipeline\n",
    "for i, field in enumerate(idealistaSchema):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        schema_list.append(field)\n",
    "schema_list.append(StructField(\"LeisureDict\", StringType(), True))#Nullable, but should not have any null values due to the pipeline (at most an empty dict)\n",
    "schema_list.append(StructField(\"IncomeDict\", StringType(), True))#Nullable, but should not have any null values due to the pipeline (at most an empty dict)\n",
    "schema = StructType(schema_list)\n",
    "df = ctx.createDataFrame(data=ilRDD.collect(), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## KPI 1; information rating of the listing, defined as the summed \"commonness\" of the provided information: sum(hasVideo, has360, hasPlan, has3Dtour, hasStaging, numPhotos, showAdress).\n",
    "## Where hasVideo: if the listing has a video / by the total amount of listings with a video\n",
    "## has360: if the listing has a video / by the total amount of listings with a 360\n",
    "## hasPlan: if the listing has a video / by the total amount of listings with a plan\n",
    "## has3Dtour: if the listing has a video / by the total amount of listings with a 3Dtour\n",
    "## hasStaging: if the listing has a video / by the total amount of listings with a Staging\n",
    "## numPhotos: numPhotos of the listing / by the average amount of photos for all listings\n",
    "## showAdress: if the listing shows the adress / by the total amount of listings that show the adress\n",
    "\n",
    "## also capture the neighbourhood name and the price in order to join them and make meaningful plots in Tableau\n",
    "\n",
    "def set_value(avg, t, feature):\n",
    "    if t[feature] == True:\n",
    "        avg['total_' + feature] = 1\n",
    "    else: avg['total_' + feature] = 0\n",
    "    return avg\n",
    "\n",
    "def init_averages(t):\n",
    "    avg = {}\n",
    "    avg = set_value(avg, t, \"hasVideo\")\n",
    "    avg = set_value(avg, t, \"has360\")\n",
    "    avg = set_value(avg, t, \"hasPlan\")\n",
    "    avg = set_value(avg, t, \"has3DTour\")\n",
    "    avg = set_value(avg, t, \"hasStaging\")\n",
    "    avg = set_value(avg, t, \"showAddress\")\n",
    "    avg[\"avgNumPhotos\"] = t[\"numPhotos\"]\n",
    "    avg[\"count\"] = 1\n",
    "    return avg\n",
    "\n",
    "def calc_totals(x, y):\n",
    "    out = {}\n",
    "    for key in x.keys():\n",
    "        out[key] = x[key] + y[key]\n",
    "    return out\n",
    "\n",
    "def calc_averages(t):\n",
    "    out = t\n",
    "    out[\"avgNumPhotos\"] = out['avgNumPhotos'] / out['count']\n",
    "    return out\n",
    "\n",
    "def calc_kpi1(t):\n",
    "    kpi1 = 0\n",
    "    for key in t[1][0][0].keys():\n",
    "        kpi1 += t[1][0][0][key]/t[1][1][key]\n",
    "    return (t[1][0][1], kpi1, t[1][0][2], t[1][0][3])\n",
    "\n",
    "KPI1rdd = df.rdd.map(lambda t: ('key', (init_averages(t), t['propertyCode'], t['NeighbourhoodName'], t['price'])))\\\n",
    "    .cache()\n",
    "\n",
    "averages = KPI1rdd.mapValues(lambda t: t[0])\\\n",
    "    .reduceByKey(lambda x, y: calc_totals(x, y))\\\n",
    "    .mapValues(lambda t: calc_averages(t)).cache()\n",
    "\n",
    "KPI1rdd = KPI1rdd.join(averages)\\\n",
    "    .map(lambda t: calc_kpi1(t))\n",
    "\n",
    "kpi1 = KPI1rdd.collect()\n",
    "\n",
    "features = ['PropertyID', 'InformationScore', 'District', 'Price']\n",
    "kpi1.sort()\n",
    "with open('KPIs/kpi1.csv', 'w') as f:\n",
    "    write = csv.writer(f, lineterminator = '\\n')\n",
    "    write.writerow(features)\n",
    "    write.writerows(kpi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##KPI 2; amenities in the neighbourhood divided by the average listing price, both for selling and renting ... if it would exist D;\n",
    "\n",
    "def leisure_str2amount(lstr):\n",
    "    amount = 0\n",
    "    if lstr != '{}':\n",
    "        splitted = lstr.split(',')\n",
    "        for item in splitted:\n",
    "            item = item[1:]\n",
    "            if \"}\" in item:\n",
    "                item = item[:-1]\n",
    "            items = item.split(\"=\")\n",
    "            amount += int(items[1])\n",
    "    return amount\n",
    "\n",
    "def init_kpi2(t):\n",
    "    return ((t['NeighbourhoodName'], t['operation']), (t['price'], leisure_str2amount(t['LeisureDict']), 1))\n",
    "\n",
    "KPI2rdd = df.rdd.map(lambda t: init_kpi2(t))\\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1], x[2]+y[2]))\\\n",
    "    .map(lambda t: (t[0][0], t[0][1], (t[1][0]/t[1][2])/(t[1][1] + 1)))\n",
    "## adding 1 to avoid devision by zero\n",
    "\n",
    "kpi2 = KPI2rdd.collect()\n",
    "\n",
    "features = ['District', 'Type of offer', 'price to leisure']\n",
    "kpi2.sort()\n",
    "with open('KPIs/kpi2.csv', 'w') as f:\n",
    "    write = csv.writer(f, lineterminator = '\\n')\n",
    "    write.writerow(features)\n",
    "    write.writerows(kpi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##KPI 3; Amount of listings posted per month per municipality\n",
    "\n",
    "KPI3rdd = df.rdd.map(lambda t: ((t['NeighbourhoodName'], t['scrap_date'][:-3]), 1))\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .map(lambda t: (t[0][0], t[0][1], t[1]))\n",
    "\n",
    "kpi3 = KPI3rdd.collect()\n",
    "\n",
    "features = ['District', 'month', 'listings']\n",
    "kpi3.sort()\n",
    "with open('KPIs/kpi3.csv', 'w') as f:\n",
    "    write = csv.writer(f, lineterminator = '\\n')\n",
    "    write.writerow(features)\n",
    "    write.writerows(kpi3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.7,0.3])\n",
    "\n",
    "indexer = StringIndexer(inputCols=[\"NeighbourhoodID\"], \n",
    "                        outputCols=[\"NeighbourhoodNum\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "ohe = OneHotEncoder(inputCols=[\"NeighbourhoodNum\"]\n",
    "                        , outputCols=[\"NeighbourhoodOhe\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=['NeighbourhoodOhe', 'price'], outputCol='feature', handleInvalid='keep')\n",
    "\n",
    "model = GeneralizedLinearRegression(featuresCol='feature', labelCol='size', family=\"gaussian\", link=\"identity\", maxIter=50, regParam=0.1)\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, ohe, vec_assembler, model])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.regParam, [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]) \\\n",
    "        .build()\n",
    "    \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"size\", predictionCol=\"prediction\", metricName='r2'),\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "results = cvModel.transform(test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"size\", predictionCol=\"prediction\", metricName='r2')\n",
    "r2 = evaluator.evaluate(results)\n",
    "print(\"r2 = %s\" % (r2))\n",
    "\n",
    "cvModel.write().overwrite().save('pipeline_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(zip(cvModel.avgMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "cvModel = CrossValidatorModel.load(\"pipeline_cv\")\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', \"venomoth.fib.upc.edu:9092\") \\\n",
    "    .option('subscribe', 'bdm_p2') \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "value_split = df.withColumn('time', split(df['value'], ',').getItem(0)) \\\n",
    "       .withColumnRenamed('NeighborhoodId', 'NeighbourhoodID') \\\n",
    "       .withColumn('NeighbourhoodID', split(df['value'], ',').getItem(1)) \\\n",
    "       .withColumn('price', split(df['value'], ',').getItem(2).cast(DoubleType()).alias(\"price\")) \\\n",
    "       .select('time', 'NeighbourhoodID', 'price') \\\n",
    "       \n",
    "predict = cvModel.transform(value_split).select(\"time\",\"NeighbourhoodId\", \"price\", \"prediction\")\n",
    "\n",
    "predict \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"predict\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from predict\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[propertyCode: string, NeighbourhoodID: string, NeighbourhoodName: string, address: string, bathrooms: bigint, country: string, detailedType: struct<subTypology:string,typology:string>, distance: string, district: string, exterior: boolean, externalReference: string, floor: string, has360: boolean, has3DTour: boolean, hasLift: boolean, hasPlan: boolean, hasStaging: boolean, hasVideo: boolean, latitude: double, longitude: double, municipality: string, neighborhood: string, newDevelopment: boolean, newDevelopmentFinished: boolean, numPhotos: bigint, operation: string, parkingSpace: struct<hasParkingSpace:boolean,isParkingSpaceIncludedInPrice:boolean,parkingSpacePrice:double>, price: double, priceByArea: double, propertyCode: string, propertyType: string, province: string, rooms: bigint, scrap_date: string, showAddress: boolean, size: double, status: string, suggestedTexts: struct<subtitle:string,title:string>, thumbnail: string, topNewDevelopment: boolean, url: string, LeisureDict: string, IncomeDict: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/19 15:58:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/06/19 15:58:57 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/06/19 15:58:57 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/06/19 15:58:57 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.6892041110109803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train,test=df.randomSplit([0.7,0.3])\n",
    "\n",
    "indexer = StringIndexer(inputCols=[\"NeighbourhoodID\"], \n",
    "                        outputCols=[\"NeighbourhoodNum\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "ohe = OneHotEncoder(inputCols=[\"NeighbourhoodNum\"]\n",
    "                        , outputCols=[\"NeighbourhoodOhe\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=['NeighbourhoodOhe', 'price'], outputCol='feature', handleInvalid='keep')\n",
    "\n",
    "model = GeneralizedLinearRegression(featuresCol='feature', labelCol='size', family=\"gaussian\", link=\"identity\", maxIter=50, regParam=0.1)\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, ohe, vec_assembler, model])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.regParam, [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]) \\\n",
    "        .build()\n",
    "    \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"size\", predictionCol=\"prediction\", metricName='r2'),\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "results = cvModel.transform(test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"size\", predictionCol=\"prediction\", metricName='r2')\n",
    "r2 = evaluator.evaluate(results)\n",
    "print(\"r2 = %s\" % (r2))\n",
    "\n",
    "cvModel.write().overwrite().save('pipeline_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7449440412363395,\n",
       " 0.7449907324497944,\n",
       " 0.7450483870901407,\n",
       " 0.7451613519451592,\n",
       " 0.7452712258669327,\n",
       " 0.7453780502380525,\n",
       " 0.7454818657906924]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7449440412363395,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.01}),\n",
       " (0.7449907324497944,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.05}),\n",
       " (0.7450483870901407,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.1}),\n",
       " (0.7451613519451592,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.2}),\n",
       " (0.7452712258669327,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.3}),\n",
       " (0.7453780502380525,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.4}),\n",
       " (0.7454818657906924,\n",
       "  {Param(parent='GeneralizedLinearRegression_a59ed1f74940', name='regParam', doc='regularization parameter (>= 0).'): 0.5})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(zip(cvModel.avgMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/19 16:00:23 WARN Utils: Your hostname, enricvm resolves to a loopback address: 127.0.1.1; using 192.168.233.128 instead (on interface ens33)\n",
      "22/06/19 16:00:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/enric/Programs/spark-3.2.1-bin-hadoop2.7/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/enric/.ivy2/cache\n",
      "The jars for the packages stored in: /home/enric/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-806429d1-316d-40ee-a95a-633ec06c511d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 538ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-806429d1-316d-40ee-a95a-633ec06c511d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/7ms)\n",
      "22/06/19 16:00:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "cvModel = CrossValidatorModel.load(\"pipeline_cv\")\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', \"venomoth.fib.upc.edu:9092\") \\\n",
    "    .option('subscribe', 'bdm_p2') \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/19 16:00:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6f5eee0d-fee0-4196-b9b6-457d8b019a84. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/19 16:00:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f34ff718220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "value_split = df.withColumn('time', split(df['value'], ',').getItem(0)) \\\n",
    "       .withColumnRenamed('NeighborhoodId', 'NeighbourhoodID') \\\n",
    "       .withColumn('NeighbourhoodID', split(df['value'], ',').getItem(1)) \\\n",
    "       .withColumn('price', split(df['value'], ',').getItem(2).cast(DoubleType()).alias(\"price\")) \\\n",
    "       .select('time', 'NeighbourhoodID', 'price') \\\n",
    "       \n",
    "predict = cvModel.transform(value_split).select(\"time\",\"NeighbourhoodId\", \"price\", \"prediction\")\n",
    "\n",
    "predict \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"predict\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------+------------------+\n",
      "|                time|NeighbourhoodId|   price|        prediction|\n",
      "+--------------------+---------------+--------+------------------+\n",
      "|2022-06-19 14:00:...|        Q980253|349953.0| 83.32995366118769|\n",
      "|2022-06-19 14:00:...|       Q3296693|234947.0|57.669583005795374|\n",
      "|2022-06-19 14:00:...|       Q2476184|259050.0|  85.3731262128205|\n",
      "|2022-06-19 14:00:...|       Q1026658|474970.0| 102.2560562879289|\n",
      "|2022-06-19 14:00:...|       Q1904302|680012.0|116.49956120502503|\n",
      "|2022-06-19 14:00:...|       Q3596096|390061.0| 90.72801928422808|\n",
      "|2022-06-19 14:00:...|       Q3297889|184940.0| 154.5006786817108|\n",
      "|2022-06-19 14:00:...|       Q1026658|590068.0|116.99798714831985|\n",
      "|2022-06-19 14:00:...|       Q3310216|198997.0|180.49572901418298|\n",
      "|2022-06-19 14:00:...|       Q2442135|798907.0| 165.2781210963214|\n",
      "|2022-06-19 14:00:...|       Q2476184|159007.0| 72.55946316857913|\n",
      "|2022-06-19 14:01:...|       Q3297056|275066.0| 82.05351756964244|\n",
      "|2022-06-19 14:01:...|       Q3750929|145013.0| 59.25427150864303|\n",
      "|2022-06-19 14:01:...|       Q3296693|152975.0| 47.17048174871039|\n",
      "|2022-06-19 14:01:...|       Q3297056|829993.0|153.12943084844989|\n",
      "+--------------------+---------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from predict\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m/home/enric/projects/mds/bdm/p2/src/test_stream.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/home/enric/projects/mds/bdm/p2/src/test_stream.ipynb#ch0000006?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39;49mstreams\u001b[39m.\u001b[39;49mactive[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mstop()\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "spark.streams.active[0].stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

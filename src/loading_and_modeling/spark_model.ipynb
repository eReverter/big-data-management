{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the df as it is done in the KPIs notebook. Same code is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and set up Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = SparkConf().setAppName(\"SparkTraining\").setMaster(\"local[*]\")\n",
    "ctx = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish the connection. If this doesn't work, comment this and use local files (snippit below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "opendata_leisureRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.opendatabcn-leisure\") \\\n",
    "    .load() \\\n",
    "    .rdd\n",
    "\n",
    "opendata_incomeRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.opendatabcn-income\") \\\n",
    "    .load() \\\n",
    "    .rdd\n",
    "\n",
    "idealista = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.idealista\") \\\n",
    "    .load() \\\n",
    "\n",
    "lookupRDD = spark.read.format(\"mongo\") \\\n",
    "    .option('uri', f\"mongodb://10.4.41.97:27017/persistent.lookup_tables\") \\\n",
    "    .load() \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the local files if the connection doesn't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idealista = ctx.read.json('./json/idealista.json')\n",
    "lookupRDD = ctx.read.json('./json/lookup.json').rdd\n",
    "opendata_incomeRDD = ctx.read.json('./json/opendata_income.json').rdd\n",
    "opendata_leisureRDD = ctx.read.json('./json/opendata_leisure.json').rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the schema of idealista to be later used when building a df out of the transformed RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idealistaSchema = idealista.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    \"\"\"\n",
    "    Flattens a nested tuple containing property data, idealista data, and other features into a single tuple.\n",
    "    \"\"\"\n",
    "    val = [t[1][0][0], t[0][0], t[0][1]]\n",
    "    for v in t[1][0][1]:\n",
    "        val.append(v)\n",
    "    return tuple(val + [t[1][1][0], t[1][1][1]])\n",
    "\n",
    "def partition_hash_neighbourhood_id(id):\n",
    "    \"\"\"\n",
    "    Returns a partition hash for a given neighborhood ID, based on its first character.\n",
    "    \"\"\"\n",
    "    val = ord(id[:1])  # Convert first character to its ASCII value\n",
    "    return val % 2\n",
    "\n",
    "def x_later_than_y(x_date, y_date):\n",
    "    \"\"\"\n",
    "    Checks if x_date is later than y_date. Dates are in the format \"YYYY_MM_DD\".\n",
    "    \"\"\"\n",
    "    xy, xm, xd = x_date.split(\"_\")\n",
    "    yy, ym, yd = y_date.split(\"_\")\n",
    "    if yy > xy: return False\n",
    "    elif ym > xm: return False\n",
    "    elif yd > xd: return False\n",
    "    else: return True\n",
    "\n",
    "def reconcile_idealista(x, y):\n",
    "    \"\"\"\n",
    "    Returns the latest record between x and y based on their date fields.\n",
    "    \"\"\"\n",
    "    if x_later_than_y(x[1], y[1]):\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "def get_partition_id(id, n=2):\n",
    "    \"\"\"\n",
    "    Returns a partition ID for a given ID by hashing it and taking modulo n.\n",
    "    \"\"\"\n",
    "    val = hash(id)\n",
    "    return val % n\n",
    "\n",
    "def merge_income_dict_count(x, y):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries, summing the values for the same keys.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    xk = list(x.keys())\n",
    "    yk = list(y.keys())\n",
    "    for key in set(xk + yk):  # Union of keys from both dictionaries\n",
    "        if key in xk and key in yk:\n",
    "            out[key] = x[key] + y[key]\n",
    "        elif key in yk:\n",
    "            out[key] = y[key]\n",
    "        elif key in xk:\n",
    "            out[key] = x[key]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lookupRDD`\n",
    "\n",
    "- Selects the neighborhood, the reconciled neighborhood name, and the numerical identifier.\n",
    "- Removes all duplicates from the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luRDD = lookupRDD.map(lambda t: (t['neighborhood'], (t['neighborhood_id'], t['neighborhood_n_reconciled'])))\\\n",
    "    .reduceByKey(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIRDD`\n",
    "\n",
    "- Selects the name of the neighborhood, the year, and the measured income level.\n",
    "- Filters out all neighborhoods labeled \"No consta\" as they do not contain useful data.\n",
    "- Partitions the data by calling `get_partition_id` on the neighborhood name.\n",
    "- Joins the data with the `lookupRDD` to obtain the neighborhood ID.\n",
    "- Transforms the data so that the neighborhood ID and reconciled name become the key of the tuple, while retaining the year and income as value entries in a dictionary.\n",
    "- Merges all dictionaries corresponding to each neighborhood into a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openIRDD = opendata_incomeRDD.map(lambda t: (t['Nom_Barri'], (t['Any'], t['√çndex RFD Barcelona = 100'])))\\\n",
    "    .filter(lambda t: t[0] != \"No consta\")\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: (t[1][1], {t[1][0][0]: t[1][0][1]}))\\\n",
    "    .reduceByKey(lambda x, y: {**x, **y})\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openLRDD`\n",
    "\n",
    "- Ignores the 'created' field as it does not accurately reflect when the amenity was actually built.\n",
    "- Selects the neighborhood and the type of amenity as a key, incrementing a count for each amenity.\n",
    "- Filters out all neighborhoods with empty names.\n",
    "- Immediately joins the data with the `lookupRDD` to simplify subsequent operations.\n",
    "- Excludes neighborhoods in the `lookupRDD` that are not present in the leisure data. A left outer join is used to ensure this.\n",
    "- Rearranges the data so the key becomes a tuple containing the ID, reconciled name, and amenity type, with a value of 1 for each amenity.\n",
    "- Performs a `reduceByKey` operation to count the total number of amenities per neighborhood, using the tuple `(neighborhood, amenity)` as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openLRDD = opendata_leisureRDD.map(lambda t: (t['addresses_neighborhood_name'],\n",
    "                                              (t['secondary_filters_name'], 1)))\\\n",
    "    .filter(lambda t: t[0] != '')\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: ((t[1][1][0], t[1][1][1]), {str(t[1][0][0]): t[1][0][1]}))\\\n",
    "    .reduceByKey(lambda x, y: merge_income_dict_count(x, y))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joinOpenRDD`\n",
    "\n",
    "- Performs a full outer join on the neighborhood ID between `openLRDD` and `openIRDD`.\n",
    "- Accounts for keys that may not appear in either dataset by setting any `None` values to empty dictionaries.\n",
    "- Utilizes `mapValues` to transform the values without changing the keys, preserving the partition information, which aids in subsequent joins with `idealistaRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinOpenRDD = openLRDD.fullOuterJoin(openIRDD)\\\n",
    "    .mapValues(lambda t: ({} if t[0] == None else t[0], t[1]))\\\n",
    "    .mapValues(lambda t: (t[0], {} if t[1] == None else t[1]))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ilRDD`\n",
    "\n",
    "- Creates rows with the property ID as the key and saves the neighborhood name along with other data.\n",
    "- Performs a `reduceByKey` operation on the property ID to remove duplicates, retaining the record with the latest date through the `reconcile_idealista` function.\n",
    "- Transforms the key to the neighborhood name for joining with `lookupRDD`.\n",
    "- Filters out records where the neighborhood name is not a string.\n",
    "- Joins with `lookupRDD` to get the neighborhood ID and reconciled name.\n",
    "- Maps the key to a tuple containing the reconciled neighborhood ID and name.\n",
    "- Joins the RDD with `joinOpenRDD`, which contains both leisure and income data.\n",
    "- Flattens all values into a single, non-nested tuple.\n",
    "- Checks for duplicates based on the latest scraping date and retains the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilRDD = idealista.rdd.map(lambda t: (t['propertyCode'], (t['neighborhood'], t['scrap_date'], t[1:])))\\\n",
    "    .reduceByKey(lambda x, y: reconcile_idealista(x, y))\\\n",
    "    .map(lambda t: (t[1][0], (t[0], t[1][2])))\\\n",
    "    .filter(lambda t: isinstance(t[0], str))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(luRDD)\\\n",
    "    .map(lambda t: (t[1][1], t[1][0]))\\\n",
    "    .partitionBy(2, lambda k: get_partition_id(k[0]))\\\n",
    "    .join(joinOpenRDD)\\\n",
    "    .map(lambda t: flatten(t)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run first part of the pipeline and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinOpenRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "schema_list = [StructField(\"propertyCode\", StringType(), False)] # Not nullable as it is an ID\n",
    "schema_list.append(StructField(\"NeighbourhoodID\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline\n",
    "schema_list.append(StructField(\"NeighbourhoodName\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline\n",
    "for i, field in enumerate(idealistaSchema):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        schema_list.append(field)\n",
    "schema_list.append(StructField(\"LeisureDict\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline (at most an empty dict)\n",
    "schema_list.append(StructField(\"IncomeDict\", StringType(), True)) # Nullable, but should not have any null values due to the pipeline (at most an empty dict)\n",
    "schema = StructType(schema_list)\n",
    "df = ctx.createDataFrame(data=ilRDD.collect(), schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Convert categorical variable \"NeighbourhoodID\" to numerical form\n",
    "indexer = StringIndexer(inputCols=[\"NeighbourhoodID\"], \n",
    "                        outputCols=[\"NeighbourhoodNum\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "# One-hot encode the numerical representation of \"NeighbourhoodID\"\n",
    "ohe = OneHotEncoder(inputCols=[\"NeighbourhoodNum\"],\n",
    "                    outputCols=[\"NeighbourhoodOhe\"],\n",
    "                    handleInvalid='keep')\n",
    "\n",
    "# Assemble feature vectors\n",
    "vec_assembler = VectorAssembler(inputCols=['NeighbourhoodOhe', 'price'],\n",
    "                                outputCol='feature',\n",
    "                                handleInvalid='keep')\n",
    "\n",
    "# Initialize the Generalized Linear Model (GLM) regressor\n",
    "model = GeneralizedLinearRegression(featuresCol='feature', labelCol='size',\n",
    "                                    family=\"gaussian\", link=\"identity\",\n",
    "                                    maxIter=50, regParam=0.1)\n",
    "\n",
    "# Create a pipeline with the above stages\n",
    "pipeline = Pipeline(stages=[indexer, ohe, vec_assembler, model])\n",
    "\n",
    "# Create the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.regParam, [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "# Initialize cross-validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"size\", predictionCol=\"prediction\", metricName='r2'),\n",
    "                          numFolds=5)  # Use 3+ folds in practice\n",
    "\n",
    "# Fit the model\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# Apply the model on the test set\n",
    "results = cvModel.transform(test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"size\", predictionCol=\"prediction\", metricName='r2')\n",
    "r2 = evaluator.evaluate(results)\n",
    "print(\"r2 = %s\" % (r2))\n",
    "\n",
    "# Save the model\n",
    "cvModel.write().overwrite().save('pipeline_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(cvModel.avgMetrics, paramGrid))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
